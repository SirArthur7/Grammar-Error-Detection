# -*- coding: utf-8 -*-
"""Exploratory Data Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i58qib7kEICuSG0_GVPBa7YWrp0Wk_9P

# EDA on the given datasets

##  Parsing the dataset
"""

import pandas as pd

# Load the dataset into a pandas dataframe.
df_train = pd.read_csv("/content/sample_data/train_data.csv")

# Report the number of sentences.
print('Number of training sentences: {:,}\n'.format(df_train.shape[0]))

# Display 10 random rows from the data.
df_train.head()

# Load the dataset into a pandas dataframe.
df_val = pd.read_csv("/content/sample_data/val_data.csv")

# Report the number of sentences.
print('Number of val sentences: {:,}\n'.format(df_val.shape[0]))

# Display 10 random rows from the data.
df_val.head()

# Load the dataset into a pandas dataframe.
df_test = pd.read_excel("/content/sample_data/test_data.xlsx")

# Report the number of sentences.
print('Number of test sentences: {:,}\n'.format(df_test.shape[0]))

# Display 10 random rows from the data.
df_test.head()

"""The two properties we actually care about are the the `input` sentence and its `label`, which is referred to as the "whether it is grammatically correct or not" (0=unacceptable, 1=acceptable).

# Performing EDA on the 3 datasets

## Downloading nltk packages
"""

import nltk
nltk.download('punkt')

"""## Train set"""

import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist

# Load the dataset
data = df_train

# Check the distribution of labels
plt.hist(data['labels'])
plt.title('Distribution of Labels')
plt.xlabel('Label')
plt.ylabel('Count')
plt.xticks([0, 1])
plt.show()

# Check the length of sentences
data['sentence_length'] = data['input'].apply(lambda x: len(word_tokenize(x)))
print('Average sentence length:', data['sentence_length'].describe())

# Check the most common words
all_words = []
for sentence in data['input']:
    words = word_tokenize(sentence)
    all_words.extend(words)

freq_dist = FreqDist(all_words)
print('Most common words:', freq_dist.most_common(10))

# Visualize the relationship between variables
plt.scatter(data['sentence_length'], data['labels'])
plt.title('Relationship between Sentence Length and Label')
plt.xlabel('Sentence Length')
plt.ylabel('Label')
plt.show()

plt.boxplot([data[data['labels'] == 0]['sentence_length'], data[data['labels'] == 1]['sentence_length']])
plt.title('Relationship between Label and Sentence Length')
plt.xlabel('Label')
plt.ylabel('Sentence Length')
plt.xticks([1, 2], ['Incorrect', 'Correct'])
plt.show()

"""## Validation set"""

data = df_val

# Check the distribution of labels
plt.hist(data['labels'])
plt.title('Distribution of Labels')
plt.xlabel('Label')
plt.ylabel('Count')
plt.xticks([0, 1])
plt.show()

# Check the length of sentences
data['sentence_length'] = data['input'].apply(lambda x: len(word_tokenize(x)))
print('Average sentence length:', data['sentence_length'].describe())

# Check the most common words
all_words = []
for sentence in data['input']:
    words = word_tokenize(sentence)
    all_words.extend(words)

freq_dist = FreqDist(all_words)
print('Most common words:', freq_dist.most_common(10))

# Visualize the relationship between variables
plt.scatter(data['sentence_length'], data['labels'])
plt.title('Relationship between Sentence Length and Label')
plt.xlabel('Sentence Length')
plt.ylabel('Label')
plt.show()

plt.boxplot([data[data['labels'] == 0]['sentence_length'], data[data['labels'] == 1]['sentence_length']])
plt.title('Relationship between Label and Sentence Length')
plt.xlabel('Label')
plt.ylabel('Sentence Length')
plt.xticks([1, 2], ['Incorrect', 'Correct'])
plt.show()

"""## Test set"""

data = df_test
# sentences = data['input'].tolist()
# apply word_tokenize to each sentence
# Check the length of sentences
df_test['input'] = df_test['input'].astype(str)
data['sentence_length'] = data['input'].apply(lambda x: len(word_tokenize(x)))
print('Average sentence length:', data['sentence_length'].describe())

# Check the most common words
all_words = []
for sentence in data['input']:
    words = word_tokenize(sentence)
    all_words.extend(words)

freq_dist = FreqDist(all_words)
print('Most common words:', freq_dist.most_common(10))

"""# Annotation errors in Train and Validation sets

## Train set
"""

df_train.iloc[4]

"""We can clearly see this is a False Positive, as the sentence is grammatically *incorrect* "sister is friend is a dentist"."""

df_train.iloc[20]

"""We can clearly see this is a False Negative, as the sentence is grammatically *correct*.

## Validation Set
"""

df_val.iloc[51]

"""We can clearly see this is a False Positive, as the sentence is grammatically *incorrect* "Becuse I", with a spelling mistake"""

df_val.iloc[20]

"""We can clearly see this is a False Negative, as the sentence is grammatically *correct*."""